---
title: "The Central Limit Theorem"
author: "Steph and John"
date: '2019-04-04'
slug: p-values
tags: p-values
categories:
- Bayes
- Frequentist
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
    keep_tex: true
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
    keep_tex: true
link-citations: yes

---


 

```{r setup, include=FALSE}
# Suppress warnings for all the packages here
# Then all of the warnings wont come up when we
# use the library(X) commands in R scripts below.
suppressPackageStartupMessages(require(tufte))
suppressPackageStartupMessages(require(tidyverse))
suppressPackageStartupMessages(library(plotly))
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)

# Set the seed for reproducibility
set.seed(1)
```


# Why are we here and how did this become a blog about theoretical statistics?

Most people find theoretical statistics garbage. 
Both Steph and John have stopped many a dinner party in its tracks with long 
and passionate discourses on the subject. 
However, we've both found a lot of value under the years in understanding 
_what's going on under the hood_. 

* When do statistics break? (We like to break things alot around here.)
    
* When can we expect them to work? (Despite evidence produced in this 
blog, they do work at least some of the time.)
    
* What does the trade off between broken applied statistics and not-broken theory 
mean in practice?
    
These are the things that we think (read: argue) about alot.

## Where can we start?

One of the most useful things to understand about theoretical statistics 
is something called the _Central Limit Theorem_. 

It's the engine in the 
[red sports car of statistics](https://www.thebayesianandthefrequentist.com/2019/03/18/p-values/). 
Many hypothesis tests need it to be true, 
a bewildering number of models require it to be working 
[and you can even have a bollocksed up polling model with big consequences if you don't have the right assumptions under the hood](https://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/).

So let's talk about the Central Limit Theorem. John's going to explain it "simply" 
and all the footnotes are Steph translating John's "simple" explanation for regular people.[^1]

[^1]: John, just to get this straight, any time you quote L'Hopital's rule, it's not a simple explanation, I'm just saying. - Steph.

## The central limit theorem

The Central Limit Theorem is actually a collection of theorems: all have the 
same general idea, but it comes in different flavours, like icecream. 

Basically, it all boils down to the fact that if you have a bunch of random observations that behave nicely, if you take a sample of those random observations, you can expect the mean of those observations to act in predictable ways - if we have enough of 
those observations.

For all the parents out there, think of it like this: your children are often 
fractious and badly behaved at home, because they're kids.[^2] But once you get 
to a certain number of your kids' friends on a playdate, all of a sudden that 
behaviour gets alot better and _they go away and entertain themselves and you 
can have a cup of tea in the middle of the chaos_. 

But you have to get to that critical mass before that's possible - if there 
aren't enough kids, or if you've got the wrong combination of kids, the wrong
environment - it doesn't work. The conditions have to be right. The central 
limit theorem is the same.[^3]

The basic recipe is as follows:

* You need a group of random numbers

* The random numbers have to be _well behaved_ - the ways they are collected and the process that generates them has to follow certain rules.

* Then good things, i.e., predictable ones, happen.

[^2]: Children are the ultimate random variables. They have their moments. 
This is a stats joke and it's Steph's fault.

[^3]: Our forthcoming parenting book _Parenting by the numbers_ is due out 
any time now. All we need is a publisher. It's going to be a hit.

To try it John's way, let's look at the mathematical version, with Steph's translation.[^4] Lets
assume that

[^4]: We should do a text classifier one day to try and work out who wrote which bits after the fact, John. A key feature for discrimination will be footnotes. - Steph

1. ${\bf X} = (X_1,\ldots,X_n)$ is a vector of $n$ random variables, i.e., a bunch of random numbers in the column of a spreadsheet or something.

2. That the random variables $X_1,\ldots,X_n$ are indepedently drawn.
This means which random numbers are chosen and their order isn't systematic 
in the sense that if I choose the first number, the second is somehow predictable.

3. They all come from the same distribution - they're all _generated in the same way_. They come from the same mechanism - whether that's height in humans, or car mileage or some other process with randomness.

4. They have a mean $\mu = {\mathbb E}(X_i)$ that exists, i.e., 
we're putting boundaries on our _generating process_ here. 
We're saying that the mean of this process can't be _infinite_ or _unreal_.[^5]

[^5]: Think of this as the assumption that requires the process generating these random numbers not to be too weird.

5. The series has a finite variance $\sigma^2 = \mbox{Var}(X_i) < \infty$[^6]

[^6]: This is the 'not totally weird assmumption, Part II'.

Based on this set of conditions, we can then start talking about the _sample mean_.

Let $\overline{X}$ denote the average of a sample of the $X_i$'s, i.e.,[^7]
$$
\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i.
$$


[^7]: This mathematics translates into normal-human readable language roughly as 'grab the Excel formula `AVG()`'.

Then for for large enough $n$ we have[^8]
$$
\overline{X} \stackrel{D}{\to} N\left(\mu,\frac{\sigma^2}{n} \right).
$$
where $\stackrel{D}{\to}$ means _convergence in distribution_. 

[^8]: Remember you need enough kids before they entertain themselves. 
That's what we mean about 'large enough n': a big enough sample size.

# Understanding the convergence in distribution bit

To give you some idea about what "convergence in distribution" means suppose
we have $N$ collections of samples (of the same size $n$ and with the 'not totally bonkers' distributional assumptions 1. to 5.),
that is,

* ${\bf X}^{(1)} = (X_1^{(1)},\ldots,X_n^{(1)})$

* ${\bf X}^{(2)} = (X_1^{(2)},\ldots,X_n^{(2)})$

* $\ldots$

* ${\bf X}^{(N)} = (X_1^{(N)},\ldots,X_n^{(N)})$

and for each of these $N$ samples we calculate the mean

* $\overline{\bf X}^{(1)} = \frac{1}{n}\sum_{i=1}^n X_i^{(1)}$

* $\overline{\bf X}^{(2)} = \frac{1}{n}\sum_{i=1}^n X_i^{(2)}$

* $\ldots$

* $\overline{\bf X}^{(N)} = \frac{1}{n}\sum_{i=1}^n X_i^{(N)}$

then the collection of these 
$$
\overline{\bf X}^{(1)},\ldots, \overline{\bf X}^{(N)}
$$
will be approximately normally distributed with mean $\mu$ and variance 
$\sigma^2/n$.

_Warning: Avengers Endgame spoilers:_ Yes, we went there. Remember how Dr Strange said he looked at all 14 billion (or something to that effect) futures? That's what we're doing. 

We're looking into a whole group of possible samples that may or may not ever happen. Because of our 'not totally weird' set of distribution assumptions, these quantities behave in predictable ways even though we don't observe them. 

We can make __inferences__ about how our own real sample (timeline, whatever) is behaving based on the bits we can see. _Even though we can't see the mechanism (distribution) that generated the randomness._ 

This is the point of the central limit theorems - to basically, be a nattily dressed wizard in a sea of technologically over-powered heroes who lack guidance. Or something.[^9]

[^9]: I want this to be a deep and meaningful metaphor on the synergies and conflicts between the machine learning and statistics disciplines, but I'm running out of laptop battery. Sorry.

# The CLT in action

The thing about the CLT is that it can make normality contagious. It can make the familiar z-statistic, built from the sample mean, act normally - even when the underlying distribution (mechanism) generating the random numbers is not normal. This is a really useful property that we use alot in hypothesis tests, models and a whole bunch of other use cases.[^10]

[^10]: There are other use cases, right John?

Let's try it out for a few non-normal distributions and see if it actually works.

Here are our distributions:

* $X_i \sim \chi_2^2$

* $X_i \sim \mbox{Weibull}(shape=1.1,scale=10)$

* $X_i \sim t_4$.

* $X_i \sim t_2$.

* $X_i \sim \mbox{Cauchy}$.

* $X_i \sim B_i \times N(-1,1)  + (1 - B_i) \times N(1,1)$
where $B_i \sim \mbox{Bernoulli}(1/2)$.

Here's what they look like:

```{r, echo = FALSE}
plot_dist <- function(distname,N=300,...)
{
  # use get function to, for example,
  # turn the string "norm" to the 
  # functions dnorm and qnorm
  ddist <- get(paste("d",distname,sep=""))
  qdist <- get(paste("q",distname,sep=""))

  # Calculate a nice grid of x values
  xlim <- qdist(c(0.01,0.99),...)
  x <- seq(xlim[1],xlim[2],,N)
  
  # Calculate the density
  y <- ddist(x,...) 
  
  return(list(x=x,y=y))
}

N <- 300
d1 <- plot_dist("chisq",N,df=2)
d2 <- plot_dist("weibull",N,shape=1.1,scale=10)
d3 <- plot_dist("t",N,df=4)
d4 <- plot_dist("t",N,df=2)
d5 <- plot_dist("t",N,df=1)
x6 <- seq(-8,8,,N)
f6 <- 0.5*dnorm(x6,-2,1) + 0.5*dnorm(x6,2,1)

x <- c(d1$x,d2$x,d3$x,d4$x,d5$x,x6)
f <- c(d1$y,d2$y,d3$y,d4$y,d5$y,f6)
dists <- c("chisq(2)","weibull(1.1,10)","t_4","t_2","Cauchy","Bimodal")
d <- rep(dists,each=N)

tib <- tibble(x,f,d)

g <- ggplot(tib,aes(x=x,y=f)) +
  geom_line(col="blue",size=1.5) + 
  facet_wrap(~d,nrow=2,ncol=3, scales="free") +
  theme_bw()
g
```
 
```{r, echo = FALSE}

# We will want to do this same thing multiple times so let's put it in a 
#generic function.

generate_plot_data <- function(N,n_val,rdist,...)
{  
  # Initialize data for N datasets of size n
  X_mat <- matrix(rdist(N*n_val, ...),N,n_val)
  
  # Calculate the means
  x_bar <- apply(X_mat,1,mean)

  # Approximate the distribution of x_bar
  dens <- density(x_bar)
  
  # Stores the data
  dat1 <- cbind(x=dens$x,y=dens$y, n_val)
  colnames(dat1) <- c("x","y","n_val")
  
  # Approximate the mean and variance of the distribution
  mu <- mean(X_mat)
  sigma2 <- var(as.vector(X_mat))
  
  # Calculate the parameters of the CLT distribution
  mu_CLT     <- mu
  sigma2_CLT <- sigma2/n_val
  sigma_CLT  <- sqrt(sigma2_CLT)
  
  # 
  xg <- seq(mu_CLT - 5*sigma_CLT,mu_CLT + 5*sigma_CLT,,1000)
  fg <- dnorm(xg,mu_CLT,sigma_CLT)
  dat2 <- cbind(xg,fg,n_val)
  colnames(dat2) <- c("x","y","n_val")

  tib1 <- as_tibble(dat1)
  tib2 <- as_tibble(dat2)
  tib1 <- tib1 %>% add_column(method="exact")
  tib2 <- tib2 %>% add_column(method="CLT")
  tib <- bind_rows(tib1,tib2)
  
  return(tib)
}
```

Let's plot the the case when $n=5$ for the Weibull distribution:

```{r, cache=TRUE, echo = FALSE}
# Call the function
tib <- generate_plot_data(N=100000,
                          n_val=5, 
                          rweibull, 
                          shape=1.1, 
                          scale=10) %>% 
  add_column(dist="weibull")

# Plot the results
g <- ggplot(tib,aes(x=x,y=y,color=method)) +
  geom_line(size=1.5) +
  theme_bw() +
  labs(color="Methods",x='X bar',y='density')
g 
```

OK so it works! 

But that was just one distribution, for one sample size.

We really want to understand what will happen for each of our distributions, over many sample sizes.


<img src="two_by_three.gif" style="width:250%">



The code used to create the animation above can be found  
[here](CLT_supp.html).

So for some of the cases above we see that it works and others that
it doesn't.

* It works for the  $\chi_2^2$, Weibull,
$t_4$, and bimodal cases. That is when the sample size
gets large enough the CLT approximation closely  matches
the sampling distribution of $\overline{X}$.

* It donesn't work for the $t_2$, the exact sampling distribution
of $\overline{X}$ is jumping around in the plot. This is because
variance of the $t_2$ distribution is not finite (see Assumption 4.)

* It donesn't work for the cauchy distribution. This is because
neither the mean of the variance exist for this distribution 
(see Assumptions 4. and 5.)
 
 

# Conclusion 

So what did we get out of all of this?

1. The central limit theorem _works_. We saw it happen. As the sample size gets big, the z-statistic acts in a predictable way.
2. It works only because we have the _not totally bonkers_ distributional assumptions in play. We also have some assumptions about how the sample to which we apply the theorem is drawn. There are certain rules and without them, the whole thing breaks. 

The most important thing to take away from this post is that while the central limit theorem is powerful, generalisable to many different scenarios and is the bedrock of alot of what we do in statistics and machine learning: _it's not infallible_. 


There are other bits we would like to discuss. We would like to give a practical
example, a closer look at rules of thumb for telling when the CLT kicks in,
and what happens to statistical tests when














 
 

