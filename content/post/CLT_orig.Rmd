---
title: "The Central Limit Theorem"
author: "Steph and John"
date: '2019-04-04'
slug: p-values
tags: p-values
categories:
- Bayes
- Frequentist
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
    keep_tex: true
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
    keep_tex: true
link-citations: yes

---


 

```{r setup, include=FALSE}
# Suppress warnings for all the packages here
# Then all of the warnings wont come up when we
# use the library(X) commands in R scripts below.
suppressPackageStartupMessages(require(tufte))
suppressPackageStartupMessages(require(tidyverse))
suppressPackageStartupMessages(library(plotly))
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)

# Set the seed for reproducibility
set.seed(1)
```


# Why are we here and how did this become a blog about theoretical statistics?

Most people find theoretical statistics garbage. Both Steph and John have stopped many a dinner party in its tracks with long and passionate discourses on the subject. However, we've both found a lot of value under the years in understanding _what's going on under the hood_. 

    * When do statistics break? (We like to break things alot around here)
    * When can we expect them to work? (Despite evidence produced in this blog, they do work at least some of the time)
    * What does the trade off between broken statistics and not-broken theory mean in practice?
    
These questions are where our focus lies.

One of the most important things to understand about theoretical statistics is something called the _Central Limit Theorem_. It's the engine in the [red sports car of statistics](https://www.thebayesianandthefrequentist.com/2019/03/18/p-values/). Many hypothesis tests need it to be true, a bewildering number of models require it to be working [and you can even have a bollocksed up polling model with big consequences if you don't have the right assumptions under the hood](https://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/), but more on that later.

So let's talk about the Central Limit Theorem. John's going to explain it simply and all the side notes are Steph translating John's simple explanation for retular people.

# Central limit theorem

The Central Limit Theorem is actually a collection of theorems: all have the same general idea, but it comes in different flavours, like icecream. 

Basically, it all boils down to the fact that if you have a bunch of random observations that behave nicely, you can expect the mean of a properly behaved sample of those observations to act in predictable ways - if we have enough of those observations.

For all the parents out there, think of it like this: your children are often fractious and badly behaved at home, because they're kids.[^1] But once you get to a certain number of your kids' friends on a playdate, all of a sudden that behaviour gets alot better and _they go away and entertain themselves and you can have a cup of tea in the middle of the chaos_. 

But you have to get to that critical mass before that's possible - if there aren't enough kids, or if you've got the wrong combination of kids, the wrong environment - it doesn't work. The conditions have to be right. The central limit theorem is the same.[^2]

The basic recipe is as follows:

    * You need a group of random numbers
    * The random numbers have to be _well behaved_ in certain ways - in the ways they are collected and the process that generates them.
    * Then good things, i.e. predictable things, happen.

[^1]: Children are the ultimate random variables. They have their moments. You can see why we're obsessed with behaviour.
[^2]: Our forthcoming parenting book _Parenting by the numbers_ is due out any time now. All we need is a publisher. It's going to be a hit.

To try it John's way, let's look at the mathematical version. Lets
assume that

(1) ${\bf X} = (X_1,\ldots,X_n)$ is a vector of $n$ random variables, i.e. just a bunch of random numbers in the column of a spreadsheet or something.

(2) That the ${\bf X_i}$ are indepedently drawn, that means the order in which those random numbers are chosen isn't systematic in the sense that if I choose the first number, the second is predictable.

(3) They all come from the same distribution- they're all _generated in the same way_. They come from the same mechanism somehow - whether that's height in humans, or car mileage or some other process with randomness.

(4) They have a mean $\mu = {\mathbb E}(X_i)$ that exists, i.e. we're putting boundaries on our _generating process_ here. We're saying that the mean of this process can't be _infinite_ or _unreal_. Think of this as the 'not totally weird' distributional assumption.

(5) The series has a finite variance $\sigma^2 = \mbox{Var}(X_i) < \infty$, this is the 'not totally weird assmumption, Part II'.

Based on this set of conditions, we can then start talking about the _sample mean_.

Let $\overline{X}$ denote the average of a sample of the $X_i$'s, i.e.,
$$
\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i 
$$

This is just maths for 'grab the Excel formula `AVG()`.

Then for for large enough $n$[^3] we have
$$
\overline{X} \stackrel{D}{\to} N\left(\mu,\frac{\sigma^2}{n} \right)
$$
where $\stackrel{D}{\to}$ means _convergence in distribution_. 

[^3]: Remember you need enough kids before they entertain themselves. That's what we mean about 'large enough n': a big enough sample size.

# Understanding the convergence in distribution bit I'M UP TO HERE JOHN

To give some intuition about what "convergence in distribution" means suppose
we have $N$ collections of samples (of the same size $n$ and assumptions (1) to (5)),
that is,

* ${\bf X}^{(1)} = (X_1^{(1)},\ldots,X_n^{(1)})$

* ${\bf X}^{(2)} = (X_1^{(2)},\ldots,X_n^{(2)})$

* $\ldots$

* ${\bf X}^{(N)} = (X_1^{(N)},\ldots,X_n^{(N)})$

and for each of these $N$ samples we calculate the mean

* $\overline{\bf X}^{(1)} = \frac{1}{n}\sum_{i=1}^n X_i^{(1)}$

* $\overline{\bf X}^{(2)} = \frac{1}{n}\sum_{i=1}^n X_i^{(2)}$

* $\ldots$

* $\overline{\bf X}^{(N)} = \frac{1}{n}\sum_{i=1}^n X_i^{(N)}$

then the collection of these $\overline{\bf X}^{(1)},\ldots, \overline{\bf X}^{(N)}$
will be approximately normally distributed with mean $\mu$ and variance 
$\sigma^2/n$.

We want to show how the CLT works.... but on a distribution which is not normally
distributed.


# The CLT in action

We will want to do this same thing multiple times so let's put it in a 
generic function.
 

```{r}
generate_plot_data <- function(N,n_val,rdist,...)
{  
  # Initialize data for N datasets of size n
  X_mat <- matrix(rdist(N*n_val, ...),N,n_val)
  
  # Calculate the means
  x_bar <- apply(X_mat,1,mean)

  # Approximate the distribution of x_bar
  dens <- density(x_bar)
  
  # Stores the data
  dat1 <- cbind(x=dens$x,y=dens$y, n_val)
  colnames(dat1) <- c("x","y","n_val")
  
  # Approximate the mean and variance of the distribution
  mu <- mean(X_mat)
  sigma2 <- var(as.vector(X_mat))
  
  # Calculate the parameters of the CLT distribution
  mu_CLT     <- mu
  sigma2_CLT <- sigma2/n_val
  sigma_CLT  <- sqrt(sigma2_CLT)
  
  # 
  xg <- seq(mu_CLT - 5*sigma_CLT,mu_CLT + 5*sigma_CLT,,1000)
  fg <- dnorm(xg,mu_CLT,sigma_CLT)
  dat2 <- cbind(xg,fg,n_val)
  colnames(dat2) <- c("x","y","n_val")

  tib1 <- as_tibble(dat1)
  tib2 <- as_tibble(dat2)
  tib1 <- tib1 %>% add_column(method="exact")
  tib2 <- tib2 %>% add_column(method="CLT")
  tib <- bind_rows(tib1,tib2)
  
  return(tib)
}
```

Plot the the case when $n=5$

```{r, cache=TRUE}
k <- 1.1
lambda <- 10

# Call the function
tib <- generate_plot_data(N=100000,n_val=2, rweibull, shape=k, scale=lambda)  %>% add_column(dist="weibull")
```

```{r}
# Plot the results
g <- ggplot(tib,aes(x=x,y=y,color=method)) +
  geom_line(size=1.5) +
  theme_bw() +
  labs(color="Methods",x='X bar',y='density')
g 
```


The code used to create the animation below can be found  
[here](CLT_supp.html).

<center>
<img src="two_by_three.gif" style="width:75%">
</center>

# A practical example

```{r}
library(Ecdat)


N <- 1000

trapint <- function(xgrid, fgrid) 
{
	ng <- length(xgrid)
	xvec <- xgrid[2:ng] - xgrid[1:(ng - 1)]
	fvec <- fgrid[1:(ng - 1)] + fgrid[2:ng]
	integ <- sum(xvec * fvec)/2
	return(integ)
}

X <- VietNamI %>% na.omit()

vn <- seq(10,200,by=5)
mErr <- matrix(0,length(vn),ncol(X))

for (k in 1:ncol(X))
{
  if (is.numeric(X[,k])) {
    
    x <- X[,k]
    n <- length(x)
    
    
    
    mu <- mean(x)
    sigma2 <- var(x)
    
    
    for (j in 1:length(vn))
    {
      x.bar <- c()
      for (i in 1:N)
      {
        inds <- sample(n,vn[j])
        x.samp <- x[inds]
        x.bar[i] <- mean(x.samp)
      }
      
      dens <- density(x.bar)
      #plot(dens)
      
      mu_CLT <- mu
      sigma2_CLT <- sigma2/vn[j]
      sigma_CLT <- sqrt(sigma2_CLT)
      
      xg <- dens$x
      yg <- dnorm(xg,mu_CLT,sigma_CLT)
      #lines(x,y,col="red",lwd=2)
    
      mErr[j,k] <- 0.5*trapint(xg,abs(yg - dens$y))
      
      cat(j,k,vn[j],colnames(X)[k],mErr[j,k],"\n")
    }
    
    plot(vn,mErr[,k],type="l") 
  }
}

```


# Why it works - Part 1
 
Assume $\mu$ and $\sigma^2$ exists.
Define
$$
Y_i = \frac{X_i - \mu}{\sigma}
$$
Define
$$
Z = \frac{1}{\sqrt{n}} \sum_{i=1}^n Y_i
$$
Using properties of expectations and variances it can be shown
$$
{\mathbb E}(Z) = 0 
\qquad \mbox{and} \qquad 
\mbox{Var}(Z) = 1. 
$$
We are going to use two special functions

* __Moment generating functions__: The moment generating function (MGF)
of a RV $X$ as a function of $t$ is defined as 
$$
M_X(t) = {\mathbb E}\left[ \exp(tX) \right]
$$

* __Cumulant generating functions__: The cumulant generating function (CGF)
of a RV $X$ as a function of $t$ is defined as 
$$
K_X(t) = \log M_X(t)
$$

Some useful properties of MGFs and CGFs include:

* The MGF and CGF uniquely identify a distribution.

* The CGF for the $X\sim N(0,1)$ is $K_X(t) = t^2/2$.

* $K_Y'(0) = {\mathbb E}(Y)$  and K_Y''(0) = \mbox{Var}(Y)$.

Now the MGF of $Z$ defined above is:
$$
\begin{array}{rl}
M_{Z}(t) 
& \displaystyle = {\mathbb E}\left[ \exp(tZ) \right]
& \qquad \mbox{(Definition)}
\\
& \displaystyle = {\mathbb E}\left[ \exp\left( \frac{t}{\sqrt{n}} \sum_{i=1}^n Y_i  \right) \right]
& \qquad \mbox{(Definition)}
\\
& \displaystyle = \prod_{i=1}^n {\mathbb E}\left[ \exp\left( \frac{t}{\sqrt{n}}  Y_i  \right) \right]
& \qquad \mbox{(Independence and properties of exp)}
\\
& \displaystyle = \prod_{i=1}^n  \left[  M_Y\left( \frac{t}{\sqrt{n}} \right) \right]^n 
& \qquad \mbox{(Identically distributed)}
\end{array} 
$$
Hence, the CGF of $Z$ satisfies:
$$
\begin{array}{rll}
K_Z(t) 
& = \log M_{Z}(t)  & \mbox{(Definition)}
\\
& = n \log M_Y\left( \frac{t}{\sqrt{n}} \right) & \mbox{(From Above)}
\\ 
& = n K_Y\left( \frac{t}{\sqrt{n}} \right) & \mbox{(Definition)}
\end{array}
$$

We can also establish that:
$$
K_Y'(0) = {\mathbb E}(Y) = 0 
\qquad \mbox{and} \qquad 
K_Y''(0) = \mbox{Var}(Y) = 1. 
$$


```{marginfigure}
__L'Hopital's rule__ (under appropriate conditions) that
that for differentiable functions $f(x)$ and $g(x)$ 
that if $\lim_{x\to c} f(x)/g(x),$
is an indetermiant form, e.g., $0/0$, $\infty/\infty$
then
$$
\displaystyle \lim_{x\to c} \frac{f(x)}{g(x)} = \lim_{x\to c} \frac{f'(x)}{g'(x)}.
$$
  
assuming the right hand side exists.
```

Now we use 
[L'Hopital's rule](https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule)
twice to obtain
$$
\begin{array}{rl}
\displaystyle \lim_{n\to\infty} K_Z(t)
& \displaystyle = \lim_{n\to\infty} n K_Y\left( \frac{t}{\sqrt{n}} \right) 
\\
& \displaystyle = \lim_{\Delta\to 0} \frac{ K_Y\left( \Delta t \right)}{\Delta^2} 
\\
& \displaystyle = \lim_{\Delta\to 0} \frac{ t K_Y'\left( \Delta t \right)}{2\Delta} \qquad \mbox{(L'Hopital's rule)} 
\\
& \displaystyle = \lim_{\Delta\to 0} \frac{ t^2 K_Y''\left( \Delta t \right)}{2} \qquad \mbox{(L'Hopital's rule)}
\\
& = \displaystyle \frac{t^2}{2}
\end{array}
$$

which is the cumulant generating function of the nomral distribution.
Hence, the CGF of $Z$ approaches that of a normal distirubiton.

 


 

<div align="center">
   <iframe width="560" height="315" src="https://player.vimeo.com/video/75089338" frameborder="0" allowfullscreen>
   </iframe>
</div>

